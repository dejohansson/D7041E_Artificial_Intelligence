{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-530ca10445fe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    114\u001b[0m                     \u001b[0mk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m                     \u001b[1;32mwhile\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mk\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mlength\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m#process right neighbors of the focus word\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 116\u001b[1;33m                         \u001b[0mword_space\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_space\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mroll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#update word embedding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    117\u001b[0m                         \u001b[0mk\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Test\\lib\\site-packages\\numpy\\core\\numeric.py\u001b[0m in \u001b[0;36mroll\u001b[1;34m(a, shift, axis)\u001b[0m\n\u001b[0;32m   1377\u001b[0m     \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1378\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1379\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mroll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshift\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1380\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1381\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\Test\\lib\\site-packages\\numpy\\core\\numeric.py\u001b[0m in \u001b[0;36mroll\u001b[1;34m(a, shift, axis)\u001b[0m\n\u001b[0;32m   1400\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mindices\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mitertools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mproduct\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mrolls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1401\u001b[0m             \u001b[0marr_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mres_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1402\u001b[1;33m             \u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mres_index\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0marr_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1403\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1404\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import text_functions as tf\n",
    "import nltk\n",
    "\n",
    "#@author: The first version of this code is the courtesy of Vadim Selyanik\n",
    "\n",
    "threshold = 15000 # Frequency threshold in the corpus ??\n",
    "dimension = 100 # Dimensionality for high-dimensional vectors\n",
    "lemmatizer = nltk.WordNetLemmatizer()  # create an instance of lemmatizer\n",
    "ones_number = 2 # number of nonzero elements in randomly generated high-dimensional vectors\n",
    "window_size = 4 #number of neighboring words to consider both back and forth. In other words number of words before/after current word\n",
    "zero_vector = np.zeros(dimension)\n",
    "test_name = \"new_toefl.txt\" # file with TOEFL dataset\n",
    "data_file_name = \"lemmatized.text\" # file with the text corpus\n",
    "\n",
    "amount_dictionary = {}\n",
    "\n",
    "# Count how many times each word appears in the corpus\n",
    "text_file = open(data_file_name, \"r\")\n",
    "for line in text_file:\n",
    "    if line != \"\\n\":\n",
    "        words = line.split()\n",
    "        for word in words:\n",
    "            if amount_dictionary.get(word) is None:\n",
    "                amount_dictionary[word] = 1\n",
    "            else:\n",
    "                amount_dictionary[word] += 1\n",
    "text_file.close()\n",
    "\n",
    "dictionary = {} #vocabulary and corresponing random high-dimensional vectors\n",
    "word_space = {} #embedings\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Create a dictionary with the assigned random high-dimensional vectors\n",
    "text_file = open(data_file_name, \"r\")\n",
    "for line in text_file: #read line in the file\n",
    "    words = line.split() # extract words from the line\n",
    "    for word in words:  # for each word\n",
    "        if dictionary.get(word) is None: # If the word was not yed added to the vocabulary\n",
    "            if amount_dictionary[word] < threshold:\n",
    "                dictionary[word] = tf.get_random_word_vector(dimension, ones_number) # assign a  \n",
    "            else:\n",
    "                dictionary[word] = np.zeros(dimension) # frequent words are assigned with empty vectors. In a way they will not contribute to the word embedding\n",
    "\n",
    "text_file.close()\n",
    "\n",
    "\n",
    "#Note that in order to save time we only create embeddings for the words needed in the TOEFL task\n",
    "\n",
    "    #Find all unique words amongst TOEFL tasks and initialize their embeddings to zeros    \n",
    "number_of_tests = 0\n",
    "text_file = open(test_name, \"r\") #open TOEFL tasks\n",
    "for line in text_file:\n",
    "        words = line.split()\n",
    "        words = [lemmatizer.lemmatize(lemmatizer.lemmatize(lemmatizer.lemmatize(word, 'v'), 'n'), 'a') for word in\n",
    "                 words] # lemmatize words in the current test\n",
    "        word_space[words[0]] = np.zeros(dimension)\n",
    "        word_space[words[1]] = np.zeros(dimension)\n",
    "        word_space[words[2]] = np.zeros(dimension)\n",
    "        word_space[words[3]] = np.zeros(dimension)\n",
    "        word_space[words[4]] = np.zeros(dimension)\n",
    "        number_of_tests += 1\n",
    "text_file.close()\n",
    "\n",
    "\n",
    "    # Processing the text to build the embeddings \n",
    "text_file = open(data_file_name, \"r\")\n",
    "lines = [[],[],[],[]] # neighboring lines\n",
    "i = 2\n",
    "while i < 4:\n",
    "        line = \"\\n\"\n",
    "        while line == \"\\n\":\n",
    "            line = text_file.readline()\n",
    "        lines[i] = line.split()\n",
    "        i += 1\n",
    "\n",
    "line = text_file.readline()\n",
    "while line != \"\":\n",
    "        if line != \"\\n\":\n",
    "            lines.append(line.split())\n",
    "            words = lines[2]\n",
    "            length = len(words)\n",
    "            i = 0\n",
    "            while i < length:\n",
    "                if not (word_space.get(words[i]) is None):\n",
    "                    k = 1\n",
    "                    word_space_vector = word_space[words[i]]\n",
    "                    while (i - k >= 0) and (k <= window_size): #process left neighbors of the focus word\n",
    "                        word_space[words[i]] = np.add(word_space[words[i]], np.roll(dictionary[words[i - k]], -1))         \n",
    "                        k += 1\n",
    "                    # Handle different situations if there was not enough neighbors on the left in the current line    \n",
    "                    if k <= window_size and (len(lines[1])>0): \n",
    "                        if len(lines[1]) < 2:\n",
    "                            if k != 1: #if one word on the left was already added\n",
    "                                word_space[words[i]] = np.add(word_space[words[i]], np.roll(dictionary[lines[1][0]], -1)) #update word embedding\n",
    "                            else:\n",
    "                                word_space[words[i]] = np.add(word_space[words[i]],\n",
    "                                                              np.roll(dictionary[lines[1][0]], -1)) #update word embedding\n",
    "                                word_space[words[i]] = np.add(word_space[words[i]],\n",
    "                                                              np.roll(dictionary[lines[0][len(lines[0]) - 1]], -1)) #update word embedding\n",
    "                        else:\n",
    "                            if k != 1:\n",
    "                                word_space[words[i]] = np.add(word_space[words[i]],\n",
    "                                                              np.roll(dictionary[lines[1][len(lines[1]) - 1]], -1)) #update word embedding\n",
    "                            else:\n",
    "                                word_space[words[i]] = np.add(word_space[words[i]],\n",
    "                                                              np.roll(dictionary[lines[1][len(lines[1]) - 1]], -1)) #update word embedding\n",
    "                                word_space[words[i]] = np.add(word_space[words[i]],\n",
    "                                                              np.roll(dictionary[lines[1][len(lines[1]) - 2]], -1)) #update word embedding\n",
    "\n",
    "                    k = 1\n",
    "                    while (i + k < length) and (k <= window_size): #process right neighbors of the focus word\n",
    "                        word_space[words[i]] = np.add(word_space[words[i]], np.roll(dictionary[words[i + k]], 1)) #update word embedding\n",
    "                        k += 1\n",
    "                    if k <= window_size:\n",
    "                        if len(lines[3]) < 2:\n",
    "                            if k != 1:\n",
    "                                word_space[words[i]] = np.add(word_space[words[i]], np.roll(dictionary[lines[3][0]], 1)) #update word embedding\n",
    "                            else:\n",
    "                                word_space[words[i]] = np.add(word_space[words[i]], np.roll(dictionary[lines[3][0]], 1)) #update word embedding\n",
    "                                word_space[words[i]] = np.add(word_space[words[i]], np.roll(dictionary[lines[4][0]], 1)) #update word embedding\n",
    "                        else:\n",
    "                            if k != 1:\n",
    "                                word_space[words[i]] = np.add(word_space[words[i]], np.roll(dictionary[lines[3][0]], 1)) #update word embedding\n",
    "                            else:\n",
    "                                word_space[words[i]] = np.add(word_space[words[i]], np.roll(dictionary[lines[3][0]], 1)) #update word embedding\n",
    "                                word_space[words[i]] = np.add(word_space[words[i]], np.roll(dictionary[lines[3][1]], 1))\n",
    "\n",
    "                i += 1\n",
    "            lines.pop(0)\n",
    "        line = text_file.readline()\n",
    "\n",
    "\n",
    "\n",
    "#Testing of the embeddings on TOEFL\n",
    "a = 0.0 # accuracy of the encodings    \n",
    "i = 0\n",
    "text_file = open(test_name, 'r')\n",
    "right_answers = 0.0 # variable for correct answers\n",
    "number_skipped_tests = 0.0 # some tests could be skipped if there are no corresponding words in the vocabulary extracted from the training corpus\n",
    "while i < number_of_tests:\n",
    "        line = text_file.readline() #read line in the file\n",
    "        words = line.split()  # extract words from the line\n",
    "        words = [lemmatizer.lemmatize(lemmatizer.lemmatize(lemmatizer.lemmatize(word, 'v'), 'n'), 'a') for word in\n",
    "                  words]  # lemmatize words in the current test\n",
    "        try:\n",
    "            \n",
    "            if not(amount_dictionary.get(words[0]) is None): # check if there word in the corpus for the query word\n",
    "                k = 1\n",
    "                while k < 5:\n",
    "                    # if amount_dictionary.get(words[k]) is None:\n",
    "                    #     word_space[words[k]] = np.random.randn(dimension)\n",
    "                    if np.array_equal(word_space[words[k]], zero_vector): # if no representation was learnt assign a random vector\n",
    "                        word_space[words[k]] = np.random.randn(dimension)\n",
    "                    k += 1\n",
    "                right_answers += tf.get_answer_mod([word_space[words[0]],word_space[words[1]],word_space[words[2]],\n",
    "                            word_space[words[3]],word_space[words[4]]]) #check if word is predicted right\n",
    "        except KeyError: # if there is no representation for the query vector than skip\n",
    "            number_skipped_tests += 1\n",
    "            print(\"skipped test: \" + str(i) + \"; Line: \" + str(words))\n",
    "        except IndexError:\n",
    "            print(i)\n",
    "            print(line)\n",
    "            print(words)\n",
    "            break\n",
    "        i += 1\n",
    "text_file.close()\n",
    "a += 100 * right_answers / number_of_tests\n",
    "print(\"Dimensions: \" + str(dimension) + \", Percentage of correct answers: \" + str(100 * right_answers / number_of_tests) + \"%\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
