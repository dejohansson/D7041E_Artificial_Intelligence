{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions of non-linear activations\n",
    "def f_sigmoid(X, deriv=False):\n",
    "    if not deriv:\n",
    "        return 1 / (1 + np.exp(-X))\n",
    "    else:\n",
    "        return f_sigmoid(X)*(1 - f_sigmoid(X))\n",
    "\n",
    "\n",
    "def f_softmax(X):\n",
    "    Z = np.sum(np.exp(X), axis=1)\n",
    "    Z = Z.reshape(Z.shape[0], 1)\n",
    "    return np.exp(X) / Z\n",
    "\n",
    "def f_relu(X):\n",
    "    return np.maximum(0, X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exit_with_err(err_str):\n",
    "    print >> sys.stderr, err_str\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functionality of a single hidden layer\n",
    "class Layer:\n",
    "    def __init__(self, size, batch_size, is_input=False, is_output=False,\n",
    "                 activation=f_sigmoid):\n",
    "        self.is_input = is_input\n",
    "        self.is_output = is_output\n",
    "\n",
    "        # Z is the matrix that holds output values\n",
    "        self.Z = np.zeros((batch_size, size[0]))\n",
    "        # The activation function is an externally defined function (with a\n",
    "        # derivative) that is stored here\n",
    "        self.activation = activation\n",
    "\n",
    "        # W is the outgoing weight matrix for this layer\n",
    "        self.W = None\n",
    "        # S is the matrix that holds the inputs to this layer\n",
    "        self.S = None\n",
    "        # D is the matrix that holds the deltas for this layer\n",
    "        self.D = None\n",
    "        # Fp is the matrix that holds the derivatives of the activation function\n",
    "        self.Fp = None\n",
    "\n",
    "        if not is_input:\n",
    "            self.S = np.zeros((batch_size, size[0]))\n",
    "            self.D = np.zeros((batch_size, size[0]))\n",
    "\n",
    "        if not is_output:\n",
    "            self.W = np.random.normal(size=size, scale=1E-4)\n",
    "\n",
    "        if not is_input and not is_output:\n",
    "            self.Fp = np.zeros((size[0], batch_size))\n",
    "\n",
    "    def forward_propagate(self):\n",
    "        if self.is_input:\n",
    "            return self.Z.dot(self.W)\n",
    "\n",
    "        self.Z = self.activation(self.S)\n",
    "        if self.is_output:\n",
    "            return self.Z\n",
    "        else:\n",
    "            # For hidden layers, we add the bias values here\n",
    "            self.Z = np.append(self.Z, np.ones((self.Z.shape[0], 1)), axis=1)\n",
    "            self.Fp = self.activation(self.S, deriv=True).T\n",
    "            return self.Z.dot(self.W)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron:\n",
    "    def __init__(self, layer_config, batch_size=100, activationOut=f_softmax):\n",
    "        self.layers = []\n",
    "        self.num_layers = len(layer_config)\n",
    "        self.minibatch_size = batch_size\n",
    "\n",
    "        for i in range(self.num_layers-1):\n",
    "            if i == 0:\n",
    "                print (\"Initializing input layer with size {0}.\".format(layer_config[i]))\n",
    "                # Here, we add an additional unit at the input for the bias\n",
    "                # weight.\n",
    "                self.layers.append(Layer([layer_config[i]+1, layer_config[i+1]],\n",
    "                                         batch_size,\n",
    "                                         is_input=True))\n",
    "            else:\n",
    "                print (\"Initializing hidden layer with size {0}.\".format(layer_config[i]))\n",
    "                # Here we add an additional unit in the hidden layers for the\n",
    "                # bias weight.\n",
    "                self.layers.append(Layer([layer_config[i]+1, layer_config[i+1]],\n",
    "                                         batch_size,\n",
    "                                         activation=f_sigmoid))\n",
    "\n",
    "        print (\"Initializing output layer with size {0}.\".format(layer_config[-1]))\n",
    "        self.layers.append(Layer([layer_config[-1], None],\n",
    "                                 batch_size,\n",
    "                                 is_output=True,\n",
    "                                 activation=activationOut))\n",
    "        print (\"Done!\")\n",
    "\n",
    "    def forward_propagate(self, data):\n",
    "        # We need to be sure to add bias values to the input\n",
    "        self.layers[0].Z = np.append(data, np.ones((data.shape[0], 1)), axis=1)\n",
    "\n",
    "        for i in range(self.num_layers-1):\n",
    "            self.layers[i+1].S = self.layers[i].forward_propagate()\n",
    "        return self.layers[-1].forward_propagate()\n",
    "\n",
    "    def backpropagate(self, yhat, labels):\n",
    "        \n",
    "        #---------------QUESTION 1 -----------#\n",
    "        #exit_with_err(\"FIND ME IN THE CODE, What is computed in the next line of code?\\n\")\n",
    "        # The error/distance between the calculated output and the \"True\" values\n",
    "        \n",
    "        self.layers[-1].D = (yhat - labels).T\n",
    "        for i in range(self.num_layers-2, 0, -1):\n",
    "            # We do not calculate deltas for the bias values\n",
    "            W_nobias = self.layers[i].W[0:-1, :]\n",
    "            \n",
    "            # ----------------QUESTION 2---------------------#\n",
    "            #exit_with_err(\"FIND ME IN THE CODE, What does this 'for' loop do?\\n\")\n",
    "            # Updates the delta for each layer through backpropagation\n",
    "            \n",
    "            self.layers[i].D = W_nobias.dot(self.layers[i+1].D) * self.layers[i].Fp\n",
    "\n",
    "    def update_weights(self, eta):\n",
    "        for i in range(0, self.num_layers-1):\n",
    "            W_grad = -eta*(self.layers[i+1].D.dot(self.layers[i].Z)).T\n",
    "            self.layers[i].W += W_grad\n",
    "\n",
    "    def evaluate(self, train_data, train_labels, test_data, test_labels,\n",
    "                 num_epochs=70, eta=0.05, eval_train=False, eval_test=True):\n",
    "\n",
    "        N_train = len(train_labels)*len(train_labels[0])\n",
    "        N_test = len(test_labels)*len(test_labels[0])\n",
    "\n",
    "        print (\"Training for {0} epochs...\".format(num_epochs))\n",
    "        for t in range(0, num_epochs):\n",
    "            out_str = \"[{0:4d}] \".format(t)\n",
    "\n",
    "            for b_data, b_labels in zip(train_data, train_labels):\n",
    "                output = self.forward_propagate(b_data)\n",
    "                self.backpropagate(output, b_labels)\n",
    "                \n",
    "                #--------------QUESTION 3----------------#\n",
    "                #exit_with_err(\"FIND ME IN THE CODE, How does weight update is implemented? What is eta?\\n\")\n",
    "                # eta = How agressive the weight update should be\n",
    "                # Dot product between the next delta and the curreent layer output, \n",
    "                # then multiply with -eta, this value is added to the current weight\n",
    "                \n",
    "                self.update_weights(eta=eta)\n",
    "\n",
    "            if eval_train:\n",
    "                errs = 0\n",
    "                for b_data, b_labels in zip(train_data, train_labels):\n",
    "                    output = self.forward_propagate(b_data)\n",
    "                    yhat = np.argmax(output, axis=1)\n",
    "                    errs += np.sum(1-b_labels[np.arange(len(b_labels)), yhat])\n",
    "\n",
    "                out_str = (\"{0} Training error: {1:.5f}\".format(out_str,\n",
    "                                                           float(errs)/N_train))\n",
    "\n",
    "            if eval_test:\n",
    "                errs = 0\n",
    "                for b_data, b_labels in zip(test_data, test_labels):\n",
    "                    output = self.forward_propagate(b_data)\n",
    "                    yhat = np.argmax(output, axis=1)\n",
    "                    errs += np.sum(1-b_labels[np.arange(len(b_labels)), yhat])\n",
    "\n",
    "                out_str = (\"{0} Test error: {1:.5f}\").format(out_str,\n",
    "                                                       float(errs)/N_test)\n",
    "\n",
    "            print (out_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_to_bit_vector(labels, nbits):\n",
    "    bit_vector = np.zeros((labels.shape[0], nbits))\n",
    "    for i in range(labels.shape[0]):\n",
    "        bit_vector[i, labels[i]] = 1.0\n",
    "\n",
    "    return bit_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batches(data, labels, batch_size, create_bit_vector=False):\n",
    "    N = data.shape[0]\n",
    "    print (\"Batch size {0}, the number of examples {1}.\".format(batch_size,N))\n",
    "\n",
    "    if N % batch_size != 0:\n",
    "        print (\"Warning in create_minibatches(): Batch size {0} does not \" \\\n",
    "              \"evenly divide the number of examples {1}.\".format(batch_size,N))\n",
    "    chunked_data = []\n",
    "    chunked_labels = []\n",
    "    idx = 0\n",
    "    while idx + batch_size <= N:\n",
    "        chunked_data.append(data[idx:idx+batch_size, :])\n",
    "        if not create_bit_vector:\n",
    "            chunked_labels.append(labels[idx:idx+batch_size])\n",
    "        else:\n",
    "            bit_vector = label_to_bit_vector(labels[idx:idx+batch_size], 10)\n",
    "            chunked_labels.append(bit_vector)\n",
    "\n",
    "        idx += batch_size\n",
    "\n",
    "    return chunked_data, chunked_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_backprop(batch_size, Train_images, Train_labels, Valid_images, Valid_labels):\n",
    "    \n",
    "    print (\"Creating data...\")\n",
    "    batched_train_data, batched_train_labels = create_batches(Train_images, Train_labels,\n",
    "                                              batch_size,\n",
    "                                              create_bit_vector=True)\n",
    "    batched_valid_data, batched_valid_labels = create_batches(Valid_images, Valid_labels,\n",
    "                                              batch_size,\n",
    "                                              create_bit_vector=True)\n",
    "    print (\"Done!\")\n",
    "\n",
    "\n",
    "    return batched_train_data, batched_train_labels,  batched_valid_data, batched_valid_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "(Xtr, Ltr), (X_test, L_test)=mnist.load_data()\n",
    "\n",
    "Xtr = Xtr.reshape(60000, 784)\n",
    "X_test = X_test.reshape(10000, 784)\n",
    "Xtr = Xtr.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "Xtr /= 255\n",
    "X_test /= 255\n",
    "print(Xtr.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# eta = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating data...\n",
      "Batch size 100, the number of examples 60000.\n",
      "Batch size 100, the number of examples 10000.\n",
      "Done!\n",
      "Initializing input layer with size 784.\n",
      "Initializing hidden layer with size 100.\n",
      "Initializing hidden layer with size 100.\n",
      "Initializing output layer with size 10.\n",
      "Done!\n",
      "Training for 70 epochs...\n",
      "[   0]  Training error: 0.58533 Test error: 0.58690\n",
      "[   1]  Training error: 0.08298 Test error: 0.07900\n",
      "[   2]  Training error: 0.06225 Test error: 0.06270\n",
      "[   3]  Training error: 0.05202 Test error: 0.05480\n",
      "[   4]  Training error: 0.03803 Test error: 0.04280\n",
      "[   5]  Training error: 0.03225 Test error: 0.03870\n",
      "[   6]  Training error: 0.02890 Test error: 0.03900\n",
      "[   7]  Training error: 0.02407 Test error: 0.03560\n",
      "[   8]  Training error: 0.02075 Test error: 0.03480\n",
      "[   9]  Training error: 0.02567 Test error: 0.03920\n",
      "[  10]  Training error: 0.01623 Test error: 0.03140\n",
      "[  11]  Training error: 0.02127 Test error: 0.03520\n",
      "[  12]  Training error: 0.02045 Test error: 0.03400\n",
      "[  13]  Training error: 0.01727 Test error: 0.03420\n",
      "[  14]  Training error: 0.01868 Test error: 0.03610\n",
      "[  15]  Training error: 0.01600 Test error: 0.03440\n",
      "[  16]  Training error: 0.01538 Test error: 0.03230\n",
      "[  17]  Training error: 0.01610 Test error: 0.03330\n",
      "[  18]  Training error: 0.01715 Test error: 0.03230\n",
      "[  19]  Training error: 0.01310 Test error: 0.03370\n",
      "[  20]  Training error: 0.01190 Test error: 0.03150\n",
      "[  21]  Training error: 0.01140 Test error: 0.03020\n",
      "[  22]  Training error: 0.01038 Test error: 0.02970\n",
      "[  23]  Training error: 0.00888 Test error: 0.02950\n",
      "[  24]  Training error: 0.00935 Test error: 0.03130\n",
      "[  25]  Training error: 0.01183 Test error: 0.03310\n",
      "[  26]  Training error: 0.01188 Test error: 0.03450\n",
      "[  27]  Training error: 0.01195 Test error: 0.03310\n",
      "[  28]  Training error: 0.01002 Test error: 0.03270\n",
      "[  29]  Training error: 0.01583 Test error: 0.03660\n",
      "[  30]  Training error: 0.01420 Test error: 0.03780\n",
      "[  31]  Training error: 0.00457 Test error: 0.02930\n",
      "[  32]  Training error: 0.00400 Test error: 0.02920\n",
      "[  33]  Training error: 0.00393 Test error: 0.02960\n",
      "[  34]  Training error: 0.01413 Test error: 0.03660\n",
      "[  35]  Training error: 0.00662 Test error: 0.02960\n",
      "[  36]  Training error: 0.01082 Test error: 0.03500\n",
      "[  37]  Training error: 0.02338 Test error: 0.04630\n",
      "[  38]  Training error: 0.01090 Test error: 0.03320\n",
      "[  39]  Training error: 0.01242 Test error: 0.03690\n",
      "[  40]  Training error: 0.00550 Test error: 0.02930\n",
      "[  41]  Training error: 0.00383 Test error: 0.02880\n",
      "[  42]  Training error: 0.00523 Test error: 0.03040\n",
      "[  43]  Training error: 0.00890 Test error: 0.03370\n",
      "[  44]  Training error: 0.00838 Test error: 0.03090\n",
      "[  45]  Training error: 0.00385 Test error: 0.02860\n",
      "[  46]  Training error: 0.00265 Test error: 0.02960\n",
      "[  47]  Training error: 0.01017 Test error: 0.03580\n",
      "[  48]  Training error: 0.00330 Test error: 0.02990\n",
      "[  49]  Training error: 0.00120 Test error: 0.02830\n",
      "[  50]  Training error: 0.00153 Test error: 0.02820\n",
      "[  51]  Training error: 0.00037 Test error: 0.02810\n",
      "[  52]  Training error: 0.00018 Test error: 0.02780\n",
      "[  53]  Training error: 0.00015 Test error: 0.02820\n",
      "[  54]  Training error: 0.00012 Test error: 0.02810\n",
      "[  55]  Training error: 0.00010 Test error: 0.02810\n",
      "[  56]  Training error: 0.00008 Test error: 0.02810\n",
      "[  57]  Training error: 0.00003 Test error: 0.02800\n",
      "[  58]  Training error: 0.00002 Test error: 0.02780\n",
      "[  59]  Training error: 0.00002 Test error: 0.02790\n",
      "[  60]  Training error: 0.00002 Test error: 0.02800\n",
      "[  61]  Training error: 0.00002 Test error: 0.02820\n",
      "[  62]  Training error: 0.00002 Test error: 0.02840\n",
      "[  63]  Training error: 0.00000 Test error: 0.02830\n",
      "[  64]  Training error: 0.00000 Test error: 0.02830\n",
      "[  65]  Training error: 0.00000 Test error: 0.02830\n",
      "[  66]  Training error: 0.00000 Test error: 0.02800\n",
      "[  67]  Training error: 0.00000 Test error: 0.02790\n",
      "[  68]  Training error: 0.00000 Test error: 0.02790\n",
      "[  69]  Training error: 0.00000 Test error: 0.02820\n",
      "Done:)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size=100;\n",
    "\n",
    "train_data, train_labels, valid_data, valid_labels=prepare_for_backprop(batch_size, Xtr, Ltr, X_test, L_test)\n",
    "\n",
    "mlp = MultiLayerPerceptron(layer_config=[784, 100, 100, 10], batch_size=batch_size)\n",
    "\n",
    "mlp.evaluate(train_data, train_labels, valid_data, valid_labels,\n",
    "             eval_train=True)\n",
    "\n",
    "print(\"Done:)\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# eta = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating data...\n",
      "Batch size 100, the number of examples 60000.\n",
      "Batch size 100, the number of examples 10000.\n",
      "Done!\n",
      "Initializing input layer with size 784.\n",
      "Initializing hidden layer with size 100.\n",
      "Initializing hidden layer with size 100.\n",
      "Initializing output layer with size 10.\n",
      "Done!\n",
      "Training for 70 epochs...\n",
      "[   0]  Training error: 0.70337 Test error: 0.70090\n",
      "[   1]  Training error: 0.64723 Test error: 0.64320\n",
      "[   2]  Training error: 0.59970 Test error: 0.59920\n",
      "[   3]  Training error: 0.45023 Test error: 0.46170\n",
      "[   4]  Training error: 0.20532 Test error: 0.19630\n",
      "[   5]  Training error: 0.11045 Test error: 0.10890\n",
      "[   6]  Training error: 0.08892 Test error: 0.08490\n",
      "[   7]  Training error: 0.07593 Test error: 0.07530\n",
      "[   8]  Training error: 0.06508 Test error: 0.06530\n",
      "[   9]  Training error: 0.05532 Test error: 0.05620\n",
      "[  10]  Training error: 0.04832 Test error: 0.05020\n",
      "[  11]  Training error: 0.04265 Test error: 0.04460\n",
      "[  12]  Training error: 0.03848 Test error: 0.03990\n",
      "[  13]  Training error: 0.03438 Test error: 0.03620\n",
      "[  14]  Training error: 0.03062 Test error: 0.03430\n",
      "[  15]  Training error: 0.02795 Test error: 0.03300\n",
      "[  16]  Training error: 0.02557 Test error: 0.03200\n",
      "[  17]  Training error: 0.02343 Test error: 0.03160\n",
      "[  18]  Training error: 0.02180 Test error: 0.03060\n",
      "[  19]  Training error: 0.02018 Test error: 0.02950\n",
      "[  20]  Training error: 0.01867 Test error: 0.02940\n",
      "[  21]  Training error: 0.01758 Test error: 0.02840\n",
      "[  22]  Training error: 0.01637 Test error: 0.02870\n",
      "[  23]  Training error: 0.01512 Test error: 0.02790\n",
      "[  24]  Training error: 0.01403 Test error: 0.02750\n",
      "[  25]  Training error: 0.01317 Test error: 0.02770\n",
      "[  26]  Training error: 0.01230 Test error: 0.02730\n",
      "[  27]  Training error: 0.01123 Test error: 0.02710\n",
      "[  28]  Training error: 0.01062 Test error: 0.02700\n",
      "[  29]  Training error: 0.00982 Test error: 0.02720\n",
      "[  30]  Training error: 0.00920 Test error: 0.02730\n",
      "[  31]  Training error: 0.00857 Test error: 0.02730\n",
      "[  32]  Training error: 0.00802 Test error: 0.02770\n",
      "[  33]  Training error: 0.00753 Test error: 0.02750\n",
      "[  34]  Training error: 0.00695 Test error: 0.02740\n",
      "[  35]  Training error: 0.00652 Test error: 0.02750\n",
      "[  36]  Training error: 0.00597 Test error: 0.02750\n",
      "[  37]  Training error: 0.00552 Test error: 0.02760\n",
      "[  38]  Training error: 0.00512 Test error: 0.02750\n",
      "[  39]  Training error: 0.00455 Test error: 0.02730\n",
      "[  40]  Training error: 0.00425 Test error: 0.02730\n",
      "[  41]  Training error: 0.00402 Test error: 0.02700\n",
      "[  42]  Training error: 0.00373 Test error: 0.02680\n",
      "[  43]  Training error: 0.00338 Test error: 0.02650\n",
      "[  44]  Training error: 0.00313 Test error: 0.02640\n",
      "[  45]  Training error: 0.00287 Test error: 0.02640\n",
      "[  46]  Training error: 0.00252 Test error: 0.02620\n",
      "[  47]  Training error: 0.00223 Test error: 0.02640\n",
      "[  48]  Training error: 0.00202 Test error: 0.02630\n",
      "[  49]  Training error: 0.00180 Test error: 0.02610\n",
      "[  50]  Training error: 0.00162 Test error: 0.02610\n",
      "[  51]  Training error: 0.00147 Test error: 0.02580\n",
      "[  52]  Training error: 0.00123 Test error: 0.02560\n",
      "[  53]  Training error: 0.00115 Test error: 0.02550\n",
      "[  54]  Training error: 0.00102 Test error: 0.02540\n",
      "[  55]  Training error: 0.00082 Test error: 0.02540\n",
      "[  56]  Training error: 0.00068 Test error: 0.02520\n",
      "[  57]  Training error: 0.00057 Test error: 0.02490\n",
      "[  58]  Training error: 0.00057 Test error: 0.02490\n",
      "[  59]  Training error: 0.00048 Test error: 0.02520\n",
      "[  60]  Training error: 0.00045 Test error: 0.02540\n",
      "[  61]  Training error: 0.00043 Test error: 0.02530\n",
      "[  62]  Training error: 0.00035 Test error: 0.02510\n",
      "[  63]  Training error: 0.00032 Test error: 0.02500\n",
      "[  64]  Training error: 0.00032 Test error: 0.02480\n",
      "[  65]  Training error: 0.00032 Test error: 0.02470\n",
      "[  66]  Training error: 0.00030 Test error: 0.02490\n",
      "[  67]  Training error: 0.00030 Test error: 0.02470\n",
      "[  68]  Training error: 0.00030 Test error: 0.02490\n",
      "[  69]  Training error: 0.00028 Test error: 0.02490\n",
      "Done:)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size=100;\n",
    "\n",
    "train_data, train_labels, valid_data, valid_labels=prepare_for_backprop(batch_size, Xtr, Ltr, X_test, L_test)\n",
    "\n",
    "mlp = MultiLayerPerceptron(layer_config=[784, 100, 100, 10], batch_size=batch_size)\n",
    "\n",
    "mlp.evaluate(train_data, train_labels, valid_data, valid_labels,\n",
    "             eval_train=True, eta=0.005)\n",
    "\n",
    "print(\"Done:)\\n\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# eta = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating data...\n",
      "Batch size 100, the number of examples 60000.\n",
      "Batch size 100, the number of examples 10000.\n",
      "Done!\n",
      "Initializing input layer with size 784.\n",
      "Initializing hidden layer with size 100.\n",
      "Initializing hidden layer with size 100.\n",
      "Initializing output layer with size 10.\n",
      "Done!\n",
      "Training for 70 epochs...\n",
      "[   0]  Training error: 0.90128 Test error: 0.90200\n",
      "[   1]  Training error: 0.90128 Test error: 0.90200\n",
      "[   2]  Training error: 0.90965 Test error: 0.91080\n",
      "[   3]  Training error: 0.88763 Test error: 0.88650\n",
      "[   4]  Training error: 0.90248 Test error: 0.90260\n",
      "[   5]  Training error: 0.90263 Test error: 0.90180\n",
      "[   6]  Training error: 0.89782 Test error: 0.89900\n",
      "[   7]  Training error: 0.89782 Test error: 0.89900\n",
      "[   8]  Training error: 0.90070 Test error: 0.89680\n",
      "[   9]  Training error: 0.90248 Test error: 0.90260\n",
      "[  10]  Training error: 0.89782 Test error: 0.89900\n",
      "[  11]  Training error: 0.90248 Test error: 0.90260\n",
      "[  12]  Training error: 0.88763 Test error: 0.88650\n",
      "[  13]  Training error: 0.90248 Test error: 0.90260\n",
      "[  14]  Training error: 0.90263 Test error: 0.90180\n",
      "[  15]  Training error: 0.89782 Test error: 0.89900\n",
      "[  16]  Training error: 0.90070 Test error: 0.89680\n",
      "[  17]  Training error: 0.90248 Test error: 0.90260\n",
      "[  18]  Training error: 0.90248 Test error: 0.90260\n",
      "[  19]  Training error: 0.90263 Test error: 0.90180\n",
      "[  20]  Training error: 0.90263 Test error: 0.90180\n",
      "[  21]  Training error: 0.90248 Test error: 0.90260\n",
      "[  22]  Training error: 0.89782 Test error: 0.89900\n",
      "[  23]  Training error: 0.90248 Test error: 0.90260\n",
      "[  24]  Training error: 0.89782 Test error: 0.89900\n",
      "[  25]  Training error: 0.90263 Test error: 0.90180\n",
      "[  26]  Training error: 0.90263 Test error: 0.90180\n",
      "[  27]  Training error: 0.90128 Test error: 0.90200\n",
      "[  28]  Training error: 0.90137 Test error: 0.90420\n",
      "[  29]  Training error: 0.89782 Test error: 0.89900\n",
      "[  30]  Training error: 0.90248 Test error: 0.90260\n",
      "[  31]  Training error: 0.90263 Test error: 0.90180\n",
      "[  32]  Training error: 0.90128 Test error: 0.90200\n",
      "[  33]  Training error: 0.90128 Test error: 0.90200\n",
      "[  34]  Training error: 0.90085 Test error: 0.89910\n",
      "[  35]  Training error: 0.90263 Test error: 0.90180\n",
      "[  36]  Training error: 0.90070 Test error: 0.89680\n",
      "[  37]  Training error: 0.90085 Test error: 0.89910\n",
      "[  38]  Training error: 0.90085 Test error: 0.89910\n",
      "[  39]  Training error: 0.90137 Test error: 0.90420\n",
      "[  40]  Training error: 0.89782 Test error: 0.89900\n",
      "[  41]  Training error: 0.89558 Test error: 0.89720\n",
      "[  42]  Training error: 0.90248 Test error: 0.90260\n",
      "[  43]  Training error: 0.90248 Test error: 0.90260\n",
      "[  44]  Training error: 0.90085 Test error: 0.89910\n",
      "[  45]  Training error: 0.90137 Test error: 0.90420\n",
      "[  46]  Training error: 0.89782 Test error: 0.89900\n",
      "[  47]  Training error: 0.90137 Test error: 0.90420\n",
      "[  48]  Training error: 0.90128 Test error: 0.90200\n",
      "[  49]  Training error: 0.90248 Test error: 0.90260\n",
      "[  50]  Training error: 0.88763 Test error: 0.88650\n",
      "[  51]  Training error: 0.90965 Test error: 0.91080\n",
      "[  52]  Training error: 0.90248 Test error: 0.90260\n",
      "[  53]  Training error: 0.90085 Test error: 0.89910\n",
      "[  54]  Training error: 0.90263 Test error: 0.90180\n",
      "[  55]  Training error: 0.89558 Test error: 0.89720\n",
      "[  56]  Training error: 0.90248 Test error: 0.90260\n",
      "[  57]  Training error: 0.89782 Test error: 0.89900\n",
      "[  58]  Training error: 0.89558 Test error: 0.89720\n",
      "[  59]  Training error: 0.90263 Test error: 0.90180\n",
      "[  60]  Training error: 0.89782 Test error: 0.89900\n",
      "[  61]  Training error: 0.90248 Test error: 0.90260\n",
      "[  62]  Training error: 0.88763 Test error: 0.88650\n",
      "[  63]  Training error: 0.89782 Test error: 0.89900\n",
      "[  64]  Training error: 0.90248 Test error: 0.90260\n",
      "[  65]  Training error: 0.89782 Test error: 0.89900\n",
      "[  66]  Training error: 0.90070 Test error: 0.89680\n",
      "[  67]  Training error: 0.89782 Test error: 0.89900\n",
      "[  68]  Training error: 0.89782 Test error: 0.89900\n",
      "[  69]  Training error: 0.90263 Test error: 0.90180\n",
      "Done:)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size=100;\n",
    "\n",
    "train_data, train_labels, valid_data, valid_labels=prepare_for_backprop(batch_size, Xtr, Ltr, X_test, L_test)\n",
    "\n",
    "mlp = MultiLayerPerceptron(layer_config=[784, 100, 100, 10], batch_size=batch_size)\n",
    "\n",
    "mlp.evaluate(train_data, train_labels, valid_data, valid_labels,\n",
    "             eval_train=True, eta=0.5)\n",
    "\n",
    "print(\"Done:)\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating data...\n",
      "Batch size 100, the number of examples 60000.\n",
      "Batch size 100, the number of examples 10000.\n",
      "Done!\n",
      "Initializing input layer with size 784.\n",
      "Initializing hidden layer with size 100.\n",
      "Initializing hidden layer with size 100.\n",
      "Initializing output layer with size 10.\n",
      "Done!\n",
      "Training for 70 epochs...\n",
      "[   0]  Training error: 0.90085 Test error: 0.89910\n",
      "[   1]  Training error: 0.88763 Test error: 0.88650\n",
      "[   2]  Training error: 0.90248 Test error: 0.90260\n",
      "[   3]  Training error: 0.90248 Test error: 0.90260\n",
      "[   4]  Training error: 0.90085 Test error: 0.89910\n",
      "[   5]  Training error: 0.89782 Test error: 0.89900\n",
      "[   6]  Training error: 0.90128 Test error: 0.90200\n",
      "[   7]  Training error: 0.89782 Test error: 0.89900\n",
      "[   8]  Training error: 0.90263 Test error: 0.90180\n",
      "[   9]  Training error: 0.89558 Test error: 0.89720\n",
      "[  10]  Training error: 0.89558 Test error: 0.89720\n",
      "[  11]  Training error: 0.90248 Test error: 0.90260\n",
      "[  12]  Training error: 0.90137 Test error: 0.90420\n",
      "[  13]  Training error: 0.88763 Test error: 0.88650\n",
      "[  14]  Training error: 0.90248 Test error: 0.90260\n",
      "[  15]  Training error: 0.90248 Test error: 0.90260\n",
      "[  16]  Training error: 0.90085 Test error: 0.89910\n",
      "[  17]  Training error: 0.90263 Test error: 0.90180\n",
      "[  18]  Training error: 0.90137 Test error: 0.90420\n",
      "[  19]  Training error: 0.90248 Test error: 0.90260\n",
      "[  20]  Training error: 0.90137 Test error: 0.90420\n",
      "[  21]  Training error: 0.88763 Test error: 0.88650\n",
      "[  22]  Training error: 0.90070 Test error: 0.89680\n",
      "[  23]  Training error: 0.90128 Test error: 0.90200\n",
      "[  24]  Training error: 0.90085 Test error: 0.89910\n",
      "[  25]  Training error: 0.90137 Test error: 0.90420\n",
      "[  26]  Training error: 0.90248 Test error: 0.90260\n",
      "[  27]  Training error: 0.90070 Test error: 0.89680\n",
      "[  28]  Training error: 0.90137 Test error: 0.90420\n",
      "[  29]  Training error: 0.89782 Test error: 0.89900\n",
      "[  30]  Training error: 0.90263 Test error: 0.90180\n",
      "[  31]  Training error: 0.90248 Test error: 0.90260\n",
      "[  32]  Training error: 0.88763 Test error: 0.88650\n",
      "[  33]  Training error: 0.89782 Test error: 0.89900\n",
      "[  34]  Training error: 0.89782 Test error: 0.89900\n",
      "[  35]  Training error: 0.90070 Test error: 0.89680\n",
      "[  36]  Training error: 0.90128 Test error: 0.90200\n",
      "[  37]  Training error: 0.90248 Test error: 0.90260\n",
      "[  38]  Training error: 0.90128 Test error: 0.90200\n",
      "[  39]  Training error: 0.90248 Test error: 0.90260\n",
      "[  40]  Training error: 0.90248 Test error: 0.90260\n",
      "[  41]  Training error: 0.90137 Test error: 0.90420\n",
      "[  42]  Training error: 0.90263 Test error: 0.90180\n",
      "[  43]  Training error: 0.90263 Test error: 0.90180\n",
      "[  44]  Training error: 0.90137 Test error: 0.90420\n",
      "[  45]  Training error: 0.89782 Test error: 0.89900\n",
      "[  46]  Training error: 0.90085 Test error: 0.89910\n",
      "[  47]  Training error: 0.90128 Test error: 0.90200\n",
      "[  48]  Training error: 0.90248 Test error: 0.90260\n",
      "[  49]  Training error: 0.89782 Test error: 0.89900\n",
      "[  50]  Training error: 0.90137 Test error: 0.90420\n",
      "[  51]  Training error: 0.90263 Test error: 0.90180\n",
      "[  52]  Training error: 0.88763 Test error: 0.88650\n",
      "[  53]  Training error: 0.90248 Test error: 0.90260\n",
      "[  54]  Training error: 0.90128 Test error: 0.90200\n",
      "[  55]  Training error: 0.90248 Test error: 0.90260\n",
      "[  56]  Training error: 0.90137 Test error: 0.90420\n",
      "[  57]  Training error: 0.90137 Test error: 0.90420\n",
      "[  58]  Training error: 0.90263 Test error: 0.90180\n",
      "[  59]  Training error: 0.90248 Test error: 0.90260\n",
      "[  60]  Training error: 0.90128 Test error: 0.90200\n",
      "[  61]  Training error: 0.90137 Test error: 0.90420\n",
      "[  62]  Training error: 0.90128 Test error: 0.90200\n",
      "[  63]  Training error: 0.90248 Test error: 0.90260\n",
      "[  64]  Training error: 0.90070 Test error: 0.89680\n",
      "[  65]  Training error: 0.88763 Test error: 0.88650\n",
      "[  66]  Training error: 0.90085 Test error: 0.89910\n",
      "[  67]  Training error: 0.90248 Test error: 0.90260\n",
      "[  68]  Training error: 0.90248 Test error: 0.90260\n",
      "[  69]  Training error: 0.89558 Test error: 0.89720\n",
      "Done:)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size=100;\n",
    "\n",
    "train_data, train_labels, valid_data, valid_labels=prepare_for_backprop(batch_size, Xtr, Ltr, X_test, L_test)\n",
    "\n",
    "mlp = MultiLayerPerceptron(layer_config=[784, 100, 100, 10], batch_size=batch_size, activationOut=f_relu)\n",
    "\n",
    "mlp.evaluate(train_data, train_labels, valid_data, valid_labels,\n",
    "             eval_train=True, eta=0.05)\n",
    "\n",
    "print(\"Done:)\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 4, 4])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.maximum(4, np.array([1, 2, 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
