{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import numpy.matlib\n",
    "from matplotlib import pyplot as plt\n",
    "import math\n",
    "import time\n",
    "from keras.datasets import mnist\n",
    "from keras.datasets import cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEuclideanDistance(single_point,array):\n",
    "    nrows, ncols, nfeatures=array.shape[0],array.shape[1], array.shape[2]\n",
    "    points=array.reshape((nrows*ncols,nfeatures))\n",
    "                         \n",
    "    dist = (points - single_point)**2\n",
    "    dist = np.sum(dist, axis=1)\n",
    "    dist = np.sqrt(dist)\n",
    "\n",
    "    dist=dist.reshape((nrows,ncols))\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mu, sigma = 0, 0.1\n",
    "A = np.random.normal(mu, sigma, 10)\n",
    "#A.shape, A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nrows,ncols,nfeatures=3,3,3\n",
    "\n",
    "#Generate coordinate system\n",
    "x,y=np.meshgrid(range(ncols),range(nrows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sgm0=2\n",
    "sgmdecay=0.05\n",
    "t=1\n",
    "sgm = sgm0 * math.exp(-t*sgmdecay);\n",
    "\n",
    "width = math.ceil(sgm*3)\n",
    "\n",
    "dist=np.array([[2,1,3],[3,2,3],[4,4,4]])\n",
    "bmurow, bmucol =np.unravel_index(np.argmin(dist, axis=None), dist.shape) \n",
    "\n",
    "g = np.exp(-((np.power(x - bmucol,2)) + (np.power(y - bmurow,2))) / (2*sgm*sgm));\n",
    "\n",
    "\n",
    "fromrow = max(0,bmurow - width);\n",
    "torow   = min(bmurow + width,nrows);\n",
    "fromcol = max(0,bmucol - width);\n",
    "tocol   = min(bmucol + width,ncols);\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = np.dstack([g[fromrow:torow,fromcol:tocol]]*nfeatures);\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SOM (dispRes, trainingData, ndim=10, nepochs=10, eta0=0.1, etadecay=0.05, sgm0=20, sgmdecay=0.05, showMode=0):\n",
    "    nfeatures=trainingData.shape[1]\n",
    "    ntrainingvectors=trainingData.shape[0]\n",
    "    \n",
    "    nrows = ndim\n",
    "    ncols = ndim\n",
    "    \n",
    "    mu, sigma = 0, 0.1\n",
    "    numpy.random.seed(int(time.time()))\n",
    "    som = np.random.normal(mu, sigma, (nrows,ncols,nfeatures))\n",
    "\n",
    "    if showMode >= 1:\n",
    "        print(\"\\nSOM features before training: \\n\")\n",
    "        \n",
    "        fig, ax=plt.subplots(nrows=nrows, ncols=ncols, figsize=(15,15))\n",
    "        \n",
    "        for k in range(nrows):\n",
    "            for l in range (ncols):\n",
    "                A=som[k,l,:].reshape((dispRes[0],dispRes[1]))\n",
    "                ax[k,l].imshow(A,cmap=\"plasma\")\n",
    "                ax[k,l].set_yticks([])\n",
    "                ax[k,l].set_xticks([])   \n",
    "    \n",
    "    #Generate coordinate system\n",
    "    x,y=np.meshgrid(range(ncols),range(nrows))\n",
    "    \n",
    "    \n",
    "    for t in range (1,nepochs+1):\n",
    "        #Compute the learning rate for the current epoch\n",
    "        eta = eta0 * math.exp(-t*etadecay);\n",
    "        \n",
    "        #Compute the variance of the Gaussian (Neighbourhood) function for the ucrrent epoch\n",
    "        sgm = sgm0 * math.exp(-t*sgmdecay);\n",
    "        \n",
    "        #Consider the width of the Gaussian function as 3 sigma\n",
    "        width = math.ceil(sgm*3);\n",
    "        \n",
    "        for ntraining in range(ntrainingvectors):\n",
    "            trainingVector = trainingData[ntraining,:];\n",
    "            \n",
    "            # Compute the Euclidean distance between the training vector and\n",
    "            # each neuron in the SOM map\n",
    "            dist = getEuclideanDistance(trainingVector, som);\n",
    "       \n",
    "            # Find 2D coordinates of the Best Matching Unit (bmu)\n",
    "            bmurow, bmucol =np.unravel_index(np.argmin(dist, axis=None), dist.shape) ;\n",
    "            \n",
    "            \n",
    "            #Generate a Gaussian function centered on the location of the bmu\n",
    "            g = np.exp(-((np.power(x - bmucol,2)) + (np.power(y - bmurow,2))) / (2*sgm*sgm));\n",
    "\n",
    "            #Determine the boundary of the local neighbourhood\n",
    "            fromrow = max(0,bmurow - width);\n",
    "            torow   = min(bmurow + width,nrows);\n",
    "            fromcol = max(0,bmucol - width);\n",
    "            tocol   = min(bmucol + width,ncols);\n",
    "\n",
    "            \n",
    "            #Get the neighbouring neurons and determine the size of the neighbourhood\n",
    "            neighbourNeurons = som[fromrow:torow,fromcol:tocol,:];\n",
    "            sz = neighbourNeurons.shape;\n",
    "            \n",
    "            #Transform the training vector and the Gaussian function into \n",
    "            # multi-dimensional to facilitate the computation of the neuron weights update\n",
    "            T = np.matlib.repmat(trainingVector,sz[0]*sz[1],1).reshape((sz[0],sz[1],nfeatures));                   \n",
    "            G = np.dstack([g[fromrow:torow,fromcol:tocol]]*nfeatures);\n",
    "\n",
    "            # Update the weights of the neurons that are in the neighbourhood of the bmu\n",
    "            neighbourNeurons = neighbourNeurons + eta * G * (T - neighbourNeurons);\n",
    "\n",
    "            \n",
    "            #Put the new weights of the BMU neighbouring neurons back to the\n",
    "            #entire SOM map\n",
    "            som[fromrow:torow,fromcol:tocol,:] = neighbourNeurons;\n",
    "            \n",
    "        if ((t == nepochs//2) & (showMode >= 1)):\n",
    "            print(\"\\nSOM features AFTER 50%: \\n\")\n",
    "        \n",
    "            fig, ax=plt.subplots(nrows=nrows, ncols=ncols, figsize=(15,15))\n",
    "\n",
    "            for k in range(nrows):\n",
    "                for l in range (ncols):\n",
    "                    A=som[k,l,:].reshape((dispRes[0],dispRes[1]))\n",
    "                    ax[k,l].imshow(A,cmap=\"plasma\")\n",
    "                    ax[k,l].set_yticks([])\n",
    "                ax[k,l].set_xticks([])\n",
    "\n",
    "    if showMode >= 1:\n",
    "        print(\"\\nSOM features AFTER training: \\n\")\n",
    "        \n",
    "        fig, ax=plt.subplots(nrows=nrows, ncols=ncols, figsize=(15,15))\n",
    "        \n",
    "        for k in range(nrows):\n",
    "            for l in range (ncols):\n",
    "                A=som[k,l,:].reshape((dispRes[0],dispRes[1]))\n",
    "                ax[k,l].imshow(A,cmap=\"plasma\")\n",
    "                ax[k,l].set_yticks([])\n",
    "            ax[k,l].set_xticks([])   \n",
    "    return som\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def parse_input_zoo_data(filename, header='infer'):\n",
    "\n",
    "    input_data = pd.read_csv(filename, header=header)\n",
    "\n",
    "    classes = input_data[17].tolist()\n",
    "    labels = input_data[0].tolist()\n",
    "    input_database = {\n",
    "        0: input_data.as_matrix([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16])\n",
    "    }\n",
    "\n",
    "    return input_database, labels, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_filename = 'data/zoo.txt'\n",
    "#input_vector_database, labels, classes = parse_input_zoo_data(input_filename,None)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#som_trained=SOM ([4,4],input_vector_database[0], ndim=10, nepochs=100, eta0=0.01, etadecay=0.05, sgm0=20, sgmdecay=0.05, showMode=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verification of correctness on the training set:\n",
    "\n",
    "\n",
    "\n",
    "def SOM_Test (trainingData, som_, classes, grid_, ConfusionMatrix, ndim=60):\n",
    "    nfeatures=trainingData.shape[1]\n",
    "    ntrainingvectors=trainingData.shape[0]\n",
    "    \n",
    "    nrows = ndim\n",
    "    ncols = ndim\n",
    "    \n",
    "    nclasses=np.max(classes)\n",
    "\n",
    "    som_cl=np.zeros((ndim,ndim,nclasses+1))\n",
    "    \n",
    "    \n",
    "    for ntraining in range(ntrainingvectors):\n",
    "        trainingVector = trainingData[ntraining,:];\n",
    "        class_of_sample= classes[ntraining]    \n",
    "        # Compute the Euclidean distance between the training vector and\n",
    "        # each neuron in the SOM map\n",
    "        dist = getEuclideanDistance(trainingVector, som_);\n",
    "       \n",
    "        # Find 2D coordinates of the Best Matching Unit (bmu)\n",
    "        bmurow, bmucol =np.unravel_index(np.argmin(dist, axis=None), dist.shape) ;\n",
    "        \n",
    "        \n",
    "        som_cl[bmurow, bmucol,class_of_sample]=som_cl[bmurow, bmucol,class_of_sample]+1\n",
    "    \n",
    "    \n",
    "    \n",
    "    for i in range (nrows):\n",
    "        for j in range (ncols):\n",
    "            grid_[i,j]=np.argmax(som_cl[i,j,:])\n",
    "\n",
    " \n",
    "    for ntraining in range(ntrainingvectors):\n",
    "        trainingVector = trainingData[ntraining,:];\n",
    "        class_of_sample= classes[ntraining]    \n",
    "        # Compute the Euclidean distance between the training vector and\n",
    "        # each neuron in the SOM map\n",
    "        dist = getEuclideanDistance(trainingVector, som_);\n",
    "       \n",
    "        # Find 2D coordinates of the Best Matching Unit (bmu)\n",
    "        bmurow, bmucol =np.unravel_index(np.argmin(dist, axis=None), dist.shape) ;\n",
    "        \n",
    "        predicted=np.argmax(som_cl[bmurow, bmucol,:])\n",
    "        ConfusionMatrix[class_of_sample-1, predicted-1]=ConfusionMatrix[class_of_sample-1, predicted-1]+1\n",
    "        \n",
    "    return grid_, ConfusionMatrix\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndim=10\n",
    "nrows=ndim\n",
    "ncols=ndim\n",
    "grid_color=np.zeros((nrows,ncols))\n",
    "#nclasses=np.max(classes)\n",
    "\n",
    "#Confusion_Matrix=np.zeros((nclasses,nclasses))\n",
    "#grid_color,Confusion_Matrix=SOM_Test (input_vector_database[0], som_trained, classes, grid_color, Confusion_Matrix, ndim)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.close()\n",
    "#plt.imshow(grid_color)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion_Matrix, input_vector_database[0].shape, np.sum(Confusion_Matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the MNIST data\n",
    "(Xtr, Ltr), (X_test, L_test)=mnist.load_data()\n",
    "num_sample = 500\n",
    "num_test = 200\n",
    "Tr_set = Xtr[:num_sample,:,:]\n",
    "Ltr_set = Ltr[:num_sample]\n",
    "test_set = X_test[:num_test,:,:]\n",
    "Ltest_set = L_test[:num_test]\n",
    "\n",
    "#Flatten\n",
    "Tr_set=Tr_set.reshape(num_sample,Tr_set.shape[1]*Tr_set.shape[2]).astype(int)\n",
    "Test_images = test_set.reshape(test_set.shape[0],test_set.shape[1]*test_set.shape[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SOM features before training: \n",
      "\n",
      "\n",
      "SOM features AFTER 50%: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "somTrained_20 = SOM([28, 28], Tr_set, ndim=20, nepochs=30, eta0=0.01, etadecay=0.05, sgm0=20, sgmdecay=0.05, showMode=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
